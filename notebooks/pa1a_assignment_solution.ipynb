{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Part A - Multi-Layer Perceptrons (MLPs)\n",
    "\n",
    "Welcome to the first assignment!\n",
    "\n",
    "You'll be implementing your own basic version of PyTorch (that we'll cleverly call `mytorch`), using nothing but [NumPy](https://numpy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's begin with a high-level recap on how NNs are trained.\n",
    "\n",
    "Modern NNs are generally trained by repeating these three steps:\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/training_forward.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/training_backward.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/training_step.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "In summary:\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/training_summary.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "We'll be implementing each of these steps roughly in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "- All code you write will be in the `mytorch/` folder.\n",
    "    - You shouldn't need to change anything in this notebook except for running the test cells.\n",
    "- Each problem has **three unit tests** to check your implementations for correctness.\n",
    "    - The first unit test of every problem is shown to you in case you want to see how it works.\n",
    "    - The last two are hidden, although we will provide error messages indicating potential issues in your code.\n",
    "    - **Make sure you pass all three tests before continuing**.\n",
    "- Code you write in other `.py` file(s) will be automatically reimported here.\n",
    "\n",
    "Finally:\n",
    "- Don't be intimidated by how long the assignment *looks*.\n",
    "    - It only looks long because we provide lots of descriptions and diagrams.\n",
    "    - The actual code you need to write is short\n",
    "        - (if you vectorize and use NumPy well, usually $\\leq$ 4 lines per problem)\n",
    "    - But the challenge is doing the code correct, and that requires *understanding the concepts*. \n",
    "\n",
    "**IMPORTANT: Make sure to run the below cell to import everything!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Make sure to run this cell, and don't modify these!\n",
    "import numpy as np\n",
    "\n",
    "# Import the code in `mytorch_solution/nn.py`\n",
    "from mytorch_solution import nn, optim\n",
    "\n",
    "# Extension to automatically update imported files if you change them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Forward Propagation\n",
    "---\n",
    "\n",
    "We'll begin by implementing everything needed to complete forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1: `Linear.forward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by implementing the forward pass of a linear layer.\n",
    "\n",
    "Open `mytorch_solution/nn.py` and find the `Linear.forward()` method. Complete it by implementing the equation below in NumPy and `return`ing its output. \n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\text{Linear}(X) = XW + b\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "    &\\text{Where $X$ is a matrix containing the input data, $W$ is the weight matrix, and $b$ is the bias vector,} \\\\\n",
    "    &\\text{and $XW$ indicates a matrix multiplication between $X$ and $W$.}\n",
    "\\end{align*}$$\n",
    "\n",
    "Here's a visualized example of the above formula just to make it clearer:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/linear_forward.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Notes/Hints**:\n",
    "- Notice that the bias is added to each row of $XW$. Thankfully, NumPy handles this automatically using [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n",
    "- While the above example and our test cases will use positive integers, typical values are small floats centered around zero.\n",
    "- The formula we provide translates pretty neatly to NumPy.\n",
    "    - Try to avoid hardcoding shapes and axes.\n",
    "- [Matrix multiplications](https://en.wikipedia.org/wiki/Matrix_multiplication) are different from [element-wise matrix multiplications](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)).\n",
    "    - For an intuitive visualization of matrix multiplications, see [here](http://matrixmultiplication.xyz/)\n",
    "    - Hint: `np.matmul()` a.k.a. `@`, [documentation here](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html). An element-wise mult is `np.mult()` a.k.a. `*`\n",
    "\n",
    "**Reminders:**\n",
    "- Make sure to run the cell above where we import everything.\n",
    "- After you complete your code, run the cell below to check that your implementation was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your answer:\n",
      "[[ 39.  46.  53.  60.]\n",
      " [ 84. 100. 116. 132.]]\n",
      "Should be equal to:\n",
      "[[ 39.  46.  53.  60.]\n",
      " [ 84. 100. 116. 132.]]\n"
     ]
    }
   ],
   "source": [
    "def test_linear_forward_1(Linear):\n",
    "    \"\"\"[Given] Just to demonstrate how you'll be graded.\n",
    "\n",
    "    Args:\n",
    "        Linear (class): the entire class imported from `nn.py` \n",
    "\n",
    "    Returns:\n",
    "        np.array: the output of passing through your Linear.forward()\n",
    "    \"\"\"\n",
    "    # Initialize layer that feeds 3 input channels to 4 neurons.\n",
    "    layer = Linear(3, 4)\n",
    "    # Weights/biases are normally initialized randomly to small floats centered around 0,\n",
    "    # but we'll manually set them like this for consistency/interpretability\n",
    "    layer.weight = np.array([[1., 2., 3., 4.],\n",
    "                             [5., 6., 7., 8.],\n",
    "                             [9., 10., 11., 12.]])\n",
    "    layer.bias = np.array([[1., 2., 3., 4.]])\n",
    "    \n",
    "    # Input array shaped (batch_size=2, in_features=3)\n",
    "    x = np.array([[1., 2., 3.],\n",
    "                  [4., 5., 6.]])\n",
    "    \n",
    "    # Run the input through Linear.forward().\n",
    "    out = layer.forward(x)\n",
    "    return out\n",
    "\n",
    "print(\"Your answer:\")\n",
    "print(test_linear_forward_1(nn.Linear))\n",
    "print(\"Should be equal to:\")\n",
    "print(np.array([[ 39.,  46.,  53.,  60.], [ 84., 100., 116., 132.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import test_linear_forward_1, test_linear_forward_2, test_linear_forward_3\n",
    "\n",
    "answer_1 = test_linear_forward_1(nn.Linear)\n",
    "answer_2 = test_linear_forward_2(nn.Linear)\n",
    "answer_3 = test_linear_forward_3(nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2: `ReLU.forward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are applied to the output of layers in order to make them non-linear. We'll begin by implementing the popular function `ReLU`.\n",
    "\n",
    "In `mytorch_solution/nn.py`, complete `ReLU.forward()` by implementing and `return`ing the value of this equation:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{ReLU}(X) =\n",
    "    \\begin{cases}\n",
    "    x & x > 0\\\\\n",
    "    0 & x \\leq 0\n",
    "    \\end{cases}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\text{Where $X$ is a matrix of the input data, and $x$ represents some entry of $X$.}\n",
    "\\end{align*}$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/relu_forward.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "**Notes**:\n",
    "- Essentially, we're zeroing out entries of $X$ that are below 0 and keeping the positive values as they are.\n",
    "- Notice that there are no trainable parameters here (no weights or biases). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_relu_forward_1(ReLU):\n",
    "    layer = ReLU()    \n",
    "    x = np.array([[-3., 1.,  0.],\n",
    "                  [ 4., 2., -5.]])\n",
    "    out = layer.forward(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import test_relu_forward_1, test_relu_forward_2, test_relu_forward_3\n",
    "\n",
    "answer_1 = test_relu_forward_1(nn.ReLU)\n",
    "answer_2 = test_relu_forward_2(nn.ReLU)\n",
    "answer_3 = test_relu_forward_3(nn.ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3: `Sequential.forward()`\n",
    "Now, let's implement `Sequential()`: this is a class that creates a feedforward network out of any layers we give it.\n",
    "\n",
    "In `mytorch_solution/nn.py`, complete `Sequential.forward()` by translating the following description to code:\n",
    "\n",
    "Pass `x` through the first layer of your network, then pass this output to the next layer, and so on. Return the output of the final layer.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/sequential_forward.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Assume**:\n",
    "- There could be an arbitrary number of layers.\n",
    "- All layers will have a `.forward()` function.\n",
    "- This method should work even if we add new layer types in the future (they'll all have `.forward()`).\n",
    "    - In other words, avoid hardcoding for class types.\n",
    "\n",
    "**Hint**: `for` loop and overwriting a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sequential_forward_1(Sequential, ReLU, Linear):\n",
    "    # Initialize list of layers and set their weights\n",
    "    model = Sequential(ReLU(), Linear(2, 3), ReLU())\n",
    "    model.layers[1].weight = np.array([[-1.,  2., -3.],\n",
    "                                       [ 5., -6.,  7.]])\n",
    "    model.layers[1].bias = np.array([[-1., 2., 3.]])\n",
    "\n",
    "    # Pass input through layers\n",
    "    x = np.array([[-3.,  0.],\n",
    "                [ 4.,  1.],\n",
    "                [-2., -1]])\n",
    "    out = model.forward(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import test_sequential_forward_1, test_sequential_forward_2, test_sequential_forward_3 \n",
    "\n",
    "answer_1 = test_sequential_forward_1(nn.Sequential, nn.ReLU, nn.Linear)\n",
    "answer_2 = test_sequential_forward_2(nn.Sequential, nn.ReLU, nn.Linear)\n",
    "answer_3 = test_sequential_forward_3(nn.Sequential, nn.ReLU, nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4: `CrossEntropyLoss.forward()`\n",
    "\n",
    "Nice work so far!\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/training_forward.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Let's quickly recap our work using the image from the intro.\n",
    "\n",
    "- So far, we've implemented the forward pass of `Linear`, `ReLU`, and `Sequential` (\"Network\" in the image).\n",
    "- Our code is currently capable of passing a batched input through a multi-layer network and getting logits.\n",
    "\n",
    "Now, we need to implement a loss function.\n",
    "\n",
    "The cross-entropy loss function measures divergence between the logits and the target labels. In other words, it estimates how well your network is doing in training (the lower the loss, the better).\n",
    "\n",
    "In `mytorch_solution/nn.py`, implement `CrossEntropyLoss.forward()` by `return`ing the value of $\\text{MeanCrossEntropy}$ below.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{CrossEntropy}(I, T) = - \\sum_{c=0}^{C-1}{T_{n,c} \\odot \\text{Log}(\\text{Softmax}(I_{n,c}))} \\\\\n",
    "    \\text{MeanCrossEntropy}(I, T) = \\frac{\\sum_{n=0}^{N-1}{\\text{CrossEntropy}(I, T)_{n}}}{N}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\text{where $C$ is the number of classes, $N$ is the batch size,} \\\\\n",
    "    &\\text{$T$ is the one-hot matrix of target labels shaped (N, C),} \\\\\n",
    "    &\\text{$I$ is the matrix of logits shaped (N, C),}\\\\\n",
    "    &\\text{and $\\odot$ is the element-wise matrix product.}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In other words:\n",
    "- $\\text{CrossEntropy(I, T)}$ calculates the loss of each observation in the batch, yielding a matrix shaped `(batch_size,)` (one loss float for each observation in the batch)\n",
    "- $\\text{MeanCrossEntropy(I, T)}$ just takes a simple average of these floats, yielding a single float.\n",
    "\n",
    "**Notes/Hints**:\n",
    "- `softmax()` is given in the `nn.py` file. Just call it when you need it.\n",
    "    - See the appendix for a more detailed description of what `softmax` does.\n",
    "- The [element-wise matrix product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) $\\odot$ in NumPy is just the `*` operator.\n",
    "- In `CrossEntropyLoss.forward()`, we've already converted the `target`s from a categorical encoding to a one-hot encoded matrix for you.\n",
    "    - See the appendix for a visualization/explanation of this.\n",
    "- The [official PyTorch documentation](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_xeloss_forward_1(CrossEntropyLoss):\n",
    "    # Initialize loss function\n",
    "    loss_function = CrossEntropyLoss()\n",
    "\n",
    "    # Logits array shaped (batch_size=2, num_classes=4)\n",
    "    logits = np.array([[-3., 2., -1., 0.],\n",
    "                       [-1., 2., -3., 4.]])\n",
    "\n",
    "    # Labels array shaped (batch_size=2,), indicates the index of each correct answer in the batch. \n",
    "    labels = np.array([3, 1])\n",
    "\n",
    "    # Get the loss value given the inputs\n",
    "    loss = loss_function.forward(logits, labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import test_xeloss_forward_1, test_xeloss_forward_2, test_xeloss_forward_3 \n",
    "\n",
    "answer_1 = test_xeloss_forward_1(nn.CrossEntropyLoss)\n",
    "answer_2 = test_xeloss_forward_2(nn.CrossEntropyLoss)\n",
    "answer_3 = test_xeloss_forward_3(nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Backpropagation\n",
    "---\n",
    "\n",
    "Great work so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Backpropagation\n",
    "Before you move on, make sure you understand these key points:\n",
    "\n",
    "**Why do we calculate gradients?**\n",
    "- The gradient of a weight or bias measures how much the loss would increase/decrease if you increase that weight/bias.\n",
    "- In particular, it measures the direction of 'steepest increase' for the loss. So moving in the *opposite* direction ('steepest *decrease*') of the gradient should decrease the loss, hopefully leading to better performance.\n",
    "\n",
    "**Backprop is literally just an implementation of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) from intro calculus.**\n",
    "- A loss function $L(I, T)$ applied to a neural network $f(X)$ is really just a big nested function $L(f(X), T)$.\n",
    "    - Backprop calculates the partial derivatives (more precisely, gradients) of $L(f(X), T)$ w.r.t. each of the weights/biases of $f$.\n",
    "- Derivative of nested functions? Chain rule.\n",
    "\n",
    "**Why do we do this backwards?**\n",
    "- Technically we could do this forwards or in other ways. But if you're curious, the reason we do it backwards is described in [this article](https://en.wikipedia.org/wiki/Automatic_differentiation#Forward_accumulation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1: `CrossEntropyLoss.backward()`\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/training_backward.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Here's that image again.\n",
    "\n",
    "Let's move backwards through the pipeline, starting with the loss function.\n",
    "\n",
    "In `mytorch_solution/nn.py`, complete `CrossEntropyLoss.backward()` by `return`ing the following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\text{CrossEntropy}(I, T)} \\text{Loss} = \\frac{\\text{Softmax}(I) - T}{N}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note: $T$ is one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_xeloss_backward_1(CrossEntropyLoss):\n",
    "    loss_function = CrossEntropyLoss()\n",
    "    logits = np.array([[-3., 2., -1., 0.],\n",
    "                       [-1., 2., -3., 4.]])\n",
    "    labels = np.array([3, 1])\n",
    "    loss_function.forward(logits, labels)\n",
    "    grad = loss_function.backward()\n",
    "    expected_grad = np.array([[ 2.82665133e-03,  4.19512254e-01,  2.08862853e-02, -4.43225190e-01],\n",
    "                              [ 2.94752177e-03, -4.40797443e-01,  3.98903693e-04,  4.37451017e-01]])\n",
    "\n",
    "    #passed = compare_to_answer(grad, expected_grad, \"CrossEntropyLoss.backward() Test 1\")\n",
    "    #return passed\n",
    "    \n",
    "    return expected_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import test_xeloss_backward_1, test_xeloss_backward_2, test_xeloss_backward_3 \n",
    "\n",
    "answer_1 = test_xeloss_backward_1(nn.CrossEntropyLoss)\n",
    "answer_2 = test_xeloss_backward_2(nn.CrossEntropyLoss)\n",
    "answer_3 = test_xeloss_backward_3(nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2.2: `Linear.backward()`\n",
    "This layer has trainable parameters (`weight` and `bias`) that you need to calculate gradients for.\n",
    "\n",
    "In the `backward()` method, we now need to accomplish three things:\n",
    "\n",
    "1. Calculate and **store** in `self.grad_weight` the gradient of the loss w.r.t. the weight matrix $W$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\nabla_W \\text{Loss} = X^T\\nabla_{\\text{Linear}(X)} \\text{Loss}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\text{Where $X^T$ is the transpose of $X$ and $\\nabla_{\\text{Linear}(X)} \\text{Loss}$ is `grad'.}\n",
    "\\end{align*}$$\n",
    "2. Calculate and **store** in `self.grad_bias` the gradient of the loss w.r.t. the bias vector $b$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\nabla_b \\text{Loss} = \\sum_{n=0}^{N-1}{\\nabla_{\\text{Linear}(X)} \\text{Loss}_n}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\text{Where $N$ is the batch\\_size, and the summation is across the batch\\_size axis.}\n",
    "\\end{align*}$$\n",
    "\n",
    "3. Calculate and `return` the gradient of the loss w.r.t. the input $X$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\nabla_X \\text{Loss} = \\nabla_{\\text{Linear}(X)} \\text{Loss} W^T\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_linear_backward_1(Linear):\n",
    "    layer = Linear(2, 4)\n",
    "    layer.weight = np.array([[ 1., 2.,  3., 2.],\n",
    "                             [-1., 4., -2., 3.]])\n",
    "    layer.bias = np.array([[1., 2., 3., 4.]])\n",
    "    layer.x = np.array([[1., -2.],\n",
    "                        [0., -6.]])\n",
    "\n",
    "    # Run the backward pass\n",
    "    grad = np.array([[1., 0.,  3., 2.],\n",
    "                     [5., 5., -1., 0.]])\n",
    "    grad_x = layer.backward(grad)\n",
    "    \n",
    "    # Need to check that the gradients of the input, weight, and bias are all correct.\n",
    "    return grad_x, layer.grad_weight, layer.grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import test_linear_backward_1, test_linear_backward_2, test_linear_backward_3 \n",
    "\n",
    "answer_1 = test_linear_backward_1(nn.Linear)\n",
    "answer_2 = test_linear_backward_2(nn.Linear)\n",
    "answer_3 = test_linear_backward_3(nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2.3: `ReLU.backward()`\n",
    "\n",
    "We'll now implement how, during backprop, `ReLU` calculates $\\nabla_X \\text{Loss}$: the gradient of the loss with respect to (\"w.r.t.\") its input $X$.\n",
    "\n",
    "Implement and `return` the value of $\\nabla_X \\text{Loss}$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\nabla_X \\text{Loss} = \\nabla_X \\text{ReLU}(X) \\odot \\nabla_{ReLU(X)} \\text{Loss} \n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\text{Where $\\odot$ is the element-wise product and $\\nabla_X \\text{ReLU}(X)$ is:}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_X \\text{ReLU}(X) =\n",
    "        \\begin{cases}\n",
    "        1 & x > 0\\\\\n",
    "        0 & x \\leq 0\n",
    "        \\end{cases}\n",
    "\\end{align*}$$\n",
    "\n",
    "**Hint 1**: $\\nabla_{ReLU(X)} \\text{Loss}$ is `grad`: the gradient of the loss w.r.t. ReLU's output.\n",
    "\n",
    "**Hint 2**: $\\nabla_X \\text{ReLU}(X)$ will be a matrix filled with 1's and 0's. It has 1's where the original input $X$ was positive, and 0's where $X$ was zero or negative. You can use `state` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_relu_backward_1(ReLU):\n",
    "    layer = ReLU()\n",
    "    layer.x = np.array([[1., -2.,  3., -4.],\n",
    "                        [5.,  6., -0.,  0.]])\n",
    "    grad = np.array([[-1.,  2., -3.,  4.],\n",
    "                     [ 0.,  6., -2.,  8.]])\n",
    "    grad_x = layer.backward(grad)\n",
    "    return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import test_relu_backward_1, test_relu_backward_2, test_relu_backward_3\n",
    "\n",
    "answer_1 = test_relu_backward_1(nn.ReLU)\n",
    "answer_2 = test_relu_backward_2(nn.ReLU)\n",
    "answer_3 = test_relu_backward_3(nn.ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4: `Sequential.backward()`\n",
    "\n",
    "Now to implement an algorithm to run backprop over the entire pipeline.\n",
    "\n",
    "In `mytorch_solution/nn.py`, complete `Sequential.backward()` by translating the following description to code:\n",
    "\n",
    "Begin backprop by getting the gradient from the `loss_function`'s backward. Then, pass this gradient to the `.backward` of the last layer, then continue passing these gradients backwards through the network until you've passed the first layer.\n",
    "\n",
    "**Note**: No need to return anything, as the purpose of backprop is to store gradients on each trainable layer\n",
    "\n",
    "**Hint**: Code should be similar to `Sequential.forward()`; you can use `reversed()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sequential_backward_1(Sequential, ReLU, Linear, CrossEntropyLoss):\n",
    "    loss_function = CrossEntropyLoss()\n",
    "    model = Sequential(ReLU(), Linear(2, 4), ReLU())\n",
    "    model.layers[1].weight = np.array([[-1., 4., -1., 4.],\n",
    "                                       [-3., 8., -5., 5.]])\n",
    "    model.layers[1].bias = np.array([[-2., 3., 1., -2.]])\n",
    "    x = np.array([[1.,  5.],\n",
    "                  [2., -3.],\n",
    "                  [4., -1]])\n",
    "    out = model.forward(x)\n",
    "    labels = np.array([0, 1, 1])\n",
    "\n",
    "    loss_function.forward(out, labels)\n",
    "    model.backward(loss_function)\n",
    "    # Return the entire model so we can check its gradients\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import test_sequential_backward_1, test_sequential_backward_2, test_sequential_backward_3\n",
    "\n",
    "answer_1 = test_sequential_backward_1(nn.Sequential, nn.ReLU, nn.Linear, nn.CrossEntropyLoss)\n",
    "answer_2 = test_sequential_backward_2(nn.Sequential, nn.ReLU, nn.Linear, nn.CrossEntropyLoss)\n",
    "answer_3 = test_sequential_backward_3(nn.Sequential, nn.ReLU, nn.Linear, nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Stochastic Gradient Descent (SGD)\n",
    "In `mytorch_solution/optim.py`, complete `SGD.step()` as described below:\n",
    "\n",
    "For each `Linear` layer in the network, we update its weight $W$ and bias $b$ like so:\n",
    " \n",
    "$$\\begin{align*}\n",
    "    W_t &= W_{t-1} - \\eta \\nabla_{W_{t-1}} \\text{Loss} \\\\\n",
    "    b_t &= b_{t-1} - \\eta \\nabla_{b_{t-1}} \\text{Loss}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "    &\\text{Where $W_t$ is the weight matrix after the update, $W_{t-1}$ is before the update,}\\\\\n",
    "    &\\text{$\\eta$ is the learning rate, and $\\nabla_{W_{t-1}} \\text{Loss}$ is the stored gradient of the weight matrix.}\\\\\n",
    "    &\\text{Same applies for $b$.}\n",
    "\\end{align*}$$\n",
    "\n",
    "**Hint**: `layers` contains both `Linear` layers that DO need updating, and `ReLU` activations that DON'T. Peek at how `SGD.zero_grad()` uses the `isinstance()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sgd_1(SGD, Sequential, Linear, ReLU):\n",
    "    model = Sequential(Linear(2, 3), ReLU())\n",
    "    model.layers[0].weight = np.array([[-3.,  2., -1.],\n",
    "                                       [ 0., -1.,  2.]])\n",
    "    model.layers[0].bias = np.array([[1., 0., -3.]])\n",
    "    model.layers[0].grad_weight = np.array([[-10.,  9., -8.],\n",
    "                                            [  7., -6.,  5.]])\n",
    "    model.layers[0].grad_bias = np.array([[-3., 3., -3.]])\n",
    "\n",
    "    # Create gradients manually, and update using them\n",
    "    lr = 0.15\n",
    "    optimizer = SGD(model, lr)\n",
    "    optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import test_sgd_1, test_sgd_2, test_sgd_3\n",
    "\n",
    "answer_1 = test_sgd_1(optim.SGD, nn.Sequential, nn.Linear, nn.ReLU)\n",
    "answer_2 = test_sgd_2(optim.SGD, nn.Sequential, nn.Linear, nn.ReLU)\n",
    "answer_3 = test_sgd_3(optim.SGD, nn.Sequential, nn.Linear, nn.ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Categorical to One-Hot Encodings\n",
    "Here's a description of [one-hot encodings](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics). Below is a simple visualization of the conversion.\n",
    "<div>\n",
    "    <img src=\"images/categorical_to_one_hot.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "## Softmax\n",
    "Article on the [Softmax function](https://en.wikipedia.org/wiki/Softmax_function).\n",
    "\n",
    "Here's a visualized example of Softmax's effect.\n",
    "<div>\n",
    "    <img src=\"images/softmax.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Notice how each row adds up to 1 after applying it, and how the values are in $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80e249822db5758e05c7a95f2378bda83bb74a36814d9a884ba3a875cd74994c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
